# Homework 7: Principal components analysis 
# 
# INSTRUCTIONS:
#   Run each step of the code below in R, and complete the problems.
#   Each problem requiring a response is indicated by "PROBLEM" in all caps.


# _____________________________________________________________________________
# Part 1: Covariance method of computing principal components

# Function to do PCA using the covariance method
pca_cov = function(X) {
  Cov_hat = cov(X)   ##calculate the covariance matrix of data
  eig = eigen(Cov_hat)  ##do eigen decomposition of the covaariance matrix to get the eigenvalue and eigenvectors.
  # Note: eigen() returns the eigvecs/vals in sorted order, from largest to smallest eigval.
  directions = eig$vectors  ##the direction of kth PC components are defined by the kth column of eigenvector matrix, which is a unit-length vector.
  scales = sqrt(abs(eig$values))  ##the scale of the kth PC components are defined by the square root of the kth diagonal elements of the matrix of eigenvalues.
  scores = X %*% directions   ##The score of X in the pc is defined as the dot product between the matrix of eigenvectors and the X matrix. The score of the nth X on the kth PC direction is the element on the nth row and kth column of the result matrix.
  return(list(scores=scores, directions=directions, scales=scales))
}

# PROBLEM 1A: Add a comment to each line of pca_cov to explain/justify it.



set.seed(1)
library(MASS)  # for generating multivariate normals

# Function to generate data from a bivariate normal with the given eigvals/vecs.
#   mu = mean vector
#   Cov = covariance matrix = ULU'
rbvn = function(n,mu,eigval1,eigval2,eigvec1,eigvec2) {
  U = cbind(eigvec1, eigvec2)
  L = diag(c(eigval1,eigval2))
  Cov = U %*% L %*% t(U)
  X = mvrnorm(n, mu, Cov)
  return(X)
}

# Generate data from a distribution with specified eigvals/eigvecs.
n = 10000
mu = c(0,0)
eigval1 = 2^2
eigval2 = 1
eigvec1 = c(3,1)/sqrt(10)
eigvec2 = c(-1,3)/sqrt(10)
X = rbvn(n, mu, eigval1, eigval2, eigvec1, eigvec2)
plot(X[,1],X[,2],xlim=c(-6,6),ylim=c(-6,6),cex=0.25)

# Do PCA
pca = pca_cov(X)

# Compare to true eigvals/vecs
eigval1
pca$scales[1]^2

eigval2
pca$scales[2]^2

eigvec1
pca$directions[,1]

eigvec2
pca$directions[,2]

# PROBLEM 1B: Based on the math/definitions, how would we expect the PC scales and directions to be related to the eigenvalues and eigenvectors?
## Answer: The PC scales should be equal to the square root of the eigenvalues, and the directions should be equal to the eigenvectors.
# PROBLEM 1C: Do the estimated eigvals/vecs above match the true eigvals/vecs exactly?  How do they differ?  Explain why they differ in this way.
## Answer: No, the estimated eigvals/vecs don't match the true eigval/vecs exactly. The estimated eigvals are very close to true values, but they are around the true values.
##         The estimated eigvecs are on the opposite directions comparing to the true directions, however, they are still represent the true directions where the data varies most.
##         And since the X is generated by random normal, thus the eigenvalue/eigenvector is slightly different with true value, but it is around true value.
# _____________________________________________________________________________
# Part 2: Visualizing high-dimensional data with PCA

# We can use the top few PCs to visualize the data.
# Let's try a slightly more interesting example. 
# The data doesn't have to come from a Gaussian.

# 20-dimensional data with two clusters
n = 100
p = 20
X = matrix(rnorm(p*n,0,1),ncol=p)
X[1:(n/2),] = X[1:(n/2),] + 1
plot(X[,1], X[,2], pch=16)  # first two data dimensions
plot(X[,1], X[,2], col=((1:n)>(n/2))+3, pch=16)

# PROBLEM 2A: Mathematically, what are the distributions of the two clusters in this example?
##Answer: The distribution of first clusters is a multivariate normal distribution with mean all equal to 1, variance of each Xp is 1, and no covariance between Xps.
#         The distribution of second clusters is a multivariate normal distribution with mean all equal to 0, variance of each Xp is 1, and no covariance between Xps.

# PROBLEM 2B: Roughly what do you expect the PC1 direction to be, based the distributions of the two clusters?
#Answer: I would expect the PC1 direction for this two dimensional clusters is [1/sqrt(20),..,1/sqrt(20)]
# PROBLEM 2C: Without the color coding, could you easily distinguish the two clusters
#   by plotting pairs of coordinates (e.g., dimension 1 versus dimension 2 as in the plots above)?
#Answer: No, it is not easy to distinguish the two clusters since their means on every coordinates don't differ too much, and the variance are same.


# Do PCA
pca = pca_cov(X)
pca$directions[,1:2]  # top two PCA directions
# Plot scores for top two PCA directions
plot(pca$scores[,1],pca$scores[,2], pch=16)
plot(pca$scores[,1],pca$scores[,2], col=((1:n)>(n/2))+3, pch=16)

# PROBLEM 2D: Can you more easily distinguish the two clusters using PC scores 1 and 2?
#             Was your guess regarding the PC1 direction correct?

#Answer:Yes, I can more easily distinguish the two clusters using top two PCs.
#       No, the direction of the PC1 is on the opposite direction of my guess.

# If PC1 represents unwanted variation, we can subtract it off:
X_adjusted = X - pca$scores[,1] %*% t(pca$directions[,1])

# PROBLEM 2E: Explain what the preceding line is doing, in terms
#   of the "best approximation" interpretation of PCA.
#Answer: The line is remove the partial value of X which is correlated to the PC1 component. By doing this, the remaining X-adujusted is not correlated with
#        the pc1 components, which is the values that correlated with variance between the clusters, and this value can give us information about the difference of X due to cluster's self-variance.


# If we run PCA after "adjusting for" PC1, the two clusters are no longer distinguishable.
pca_adj = pca_cov(X_adjusted)
plot(pca_adj$scores[,1],pca_adj$scores[,2], col=((1:n)>(n/2))+3, pch=16)

# PROBLEM 2F: 
#    What is a real-world example where we would want to make the clusters indistinguishable? (Give a one sentence answer.)
#Answer: We want to estimate whether confounding covariates is controlled in two groups of patients in the randomized clinical trial.
#    What is a real-world example where we would NOT want to make the clusters indistinguishable? (Give a one sentence answer.)
#Answer: We want to estimate whether there exists subgroup differenc in a group data.
# PROBLEM 2G: In pca_adj, the last PC scale is very small.  Why?
pca_adj$scales[20]
#Answer: This is because the variance on that direction has been subtracted by adjusted the X. In fact, that PC component is the top1 PC components of the original data.



# _____________________________________________________________________________
# Part 3: Comparing pca_cov (our version) to prcomp (built-in R version).

# This part uses the same X matrix as in part 2.

# Compare with prcomp, the built-in R version of PCA:
pca = pca_cov(X)  # our version
pc = prcomp(X)  # built-in R version

pca$scales  # we called them "scales"
pc$sdev  # prcomp calls them "sdev"

pca$directions[1:5,1:5]  # we called them "directions"
pc$rotation[1:5,1:5]  # prcomp calls them "rotation"
colSums(pca$directions * pc$rotation)

# PROBLEM 3A: What does a 1 or -1 mean in the output of the preceding line?
#             Why do we get -1 sometimes?  Does that mean that pca_cov is doing something wrong?
#Answer: 1 means the directions of two vectors are same, and -1 means the directions of two vectors are opposite.
#This is because the eigenvalues of those vector sum to 0, and have same absolute value. This may due to the decomposition procedure when computing the eigenvalues and 
#eigenvectors. No, this doesn't mean the pca_cov is wrong, in fact the vectors represent same direction line.


pca$scores[1:5,1:5]  # we called them "scores"
pc$x[1:5,1:5]  # prcomp calls them "x"

# Our scores differ from prcomp's "x" since prcomp computes centered scores:
colMeans(pc$x)

# We can compute centered scores using:
centered_scores = scale(pca$scores,center=T,scale=F)
centered_scores[1:5,1:5]

# PROBLEM 3B: Is centered_scores equal to pc$x or not? If not, in what way do they differ?
#No, they are not equal. for the PCs whose directions are opposite, the scores of X on those directions are also have different symbols,
# but the abosulte values of those scores are same.


# _____________________________________________________________________________
# Part 4: SVD method of computing PCA

# If ncol(X) is large (e.g., p>n), then the covariance method is slow.
# In this case, the SVD method is often used instead of the covariance method.
# A nice thing about the SVD method is that it more easily allows us to
# compute only the top few PCs, by using a truncated SVD.

# PROBLEM 4A: Complete the function pca_svd by replacing the parts indicated ___YOUR_ANSWER_HERE___.

# Do PCA using the SVD method
pca_svd = function(X, number_of_PCs=min(nrow(X),ncol(X))) {
  # Center to zero mean:
  X_centered = scale(X, center=T, scale=F)
  
  # SVD: X=UDV' (approximately if truncated SVD). 
  SVD = svd(X_centered, nu=number_of_PCs, nv=number_of_PCs)  # nu,nv specify truncation of rows,cols
  # Note: svd() returns U,D,V in sorted order, from largest to smallest singular value.
  
  directions = SVD$v
  
  scores = X%*%directions
  
  return(list(scores=scores, directions=directions))
}



# This part uses the same X matrix as in part 2.

# Run the two methods
a = pca_cov(X)
b = pca_svd(X)

a$directions[1:5,1:5]  # look at the first few entries
b$directions[1:5,1:5]  # look at the first few entries
colSums(a$directions * b$directions)

a$scores[1:5,1:5]  # look at the first few entries
b$scores[1:5,1:5]  # look at the first few entries

# PROBLEM 4B: Describe any differences you see.  If there are differences, are they justifiable or not?
# Answer: For the directions, there are still some pc directions on the opposite direction. They are justifiable.
# For the scores, they are different. After centerized the X, the scores are same for absolute value, and they are have different symbol
# for the PCs whose directions are opposite. these difference are have same reasons as described above. 


# ___________________________________________________________________________
# Part 5: Genotype example - PCA on high-dimensional data sets

# For high-dimensional data sets, computing only the top few PCs is much faster 
# than computing all of the PCs.  The truncated SVD facilitates this.

# Simulate genotype data from three populations
p = 10000  # number of loci
n1 = 400; n2 = 300; n3 = 300  # sizes of populations 1, 2, and 3
# Simulate population 1 allele frequency at each locus:
prob1 = runif(p)
# Make the population 2 allele frequency different at 40 randomly chosen loci:
prob2 = prob1; subset2 = sample(p,40); prob2[subset2] = runif(40)
# Make the population 3 allele frequency different at 80 randomly chosen loci:
prob3 = prob1; subset3 = sample(p,80); prob3[subset3] = runif(80)

# PROBLEM 5A: What percentage of loci have different allele frequency in:
#    (i) population 1 versus population 2?
# Answer: 40/10000=0.4% percentage of loci have different allele frequency.
#    (ii) population 1 versus population 3?
# Answer: 80/10000=0.8% percentage of loci have different allel frequency.

# Generate allele counts as Binomial(2,prob):
X1 = matrix(rbinom(n1*p, 2, prob1), ncol=p, byrow=T)  # simulated genotypes for subjects in population 1
X2 = matrix(rbinom(n2*p, 2, prob2), ncol=p, byrow=T)  # simulated genotypes for subjects in population 2
X3 = matrix(rbinom(n3*p, 2, prob3), ncol=p, byrow=T)  # simulated genotypes for subjects in population 3

# Put the data into a single n-by-p matrix X, where
#   X[i,j] = number of copies of allele A at locus j for subject i.
X = rbind(X1, X2, X3)
n = nrow(X)  # number of subjects

X[1:5,1:10]  # look at the first few entries

# Visualize the data for the first 50 loci and first 500 loci:
image(X[,1:50])
image(X[,1:500])  # (Be patient, this may take a few seconds to display...)

# PROBLEM 5B: 
#    In these image plots, which subsets of columns correspond to populations 1, 2, and 3?
#Answer: The poulation 1 is represented by (0.0-0.4), the population 2 is represented by (0.4-0.7), and the population 3 is represented by(0.7-1.0) of columns.
#    From these image plots, can you easily see which subjects belong
#    to different populations?  Why or why not?  Can you easily see which loci have 
#    different allele frequencies in different populations?  Why or why not?
#Answer:  No, it is not easily see the different populations, since there are only 0.4% of allels has different frequency in population 2 and 0.8% in population 3
#comparing to the population 1. 
# For some allels, we can see that there are difference between populations, however, it is not easily to be observed. Since there are 10000
# different allels and we only showed 500 allels in the image, which is already hard to observe.

# Just calling pca_svd(X) would take a long time to run.
# By specifying number_of_PCs=2, we can use the truncated SVD to more quickly compute the top 2 PCs:
pca = pca_svd(X, number_of_PCs=2)

# PROBLEM 5C: Make a scatterplot of the scores for the top two PCA directions, color coding the subjects
#    according to their true population.  Describe what you see.  Explain how this would be helpful if 
#    you didn't know how many populations there were or which subjects belonged to which population.
#    Do the distances between the populations in this scatterplot tell you anything useful about how
#    many loci have different allele frequencies between each pair of populations?
pca$Colour[1:400]="red"
pca$Colour[401:700]="black"
pca$Colour[701:1000]="blue"
plot(pca$scores[,1],pca$scores[,2],col=pca$Colour,pch=16)

#From the plot, we can see that there are three clusters which are separated clearly by top2 PCs. After we do the PCA, we can see that
# the populations are separated, and it is very easy to distinguish which subject belongs to which population, by calculating its top2 pc scores
# and plot and comparing to the cluster plot.  Yes, as the number of the different allele frequencies increase, the distance between the 
#clusters on the scatterplot is increasing.








